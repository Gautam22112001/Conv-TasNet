{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"music-source-separation.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPyuVGHQ8FQHN/mwvhgtTU7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"e5VTKssvFchI"},"source":["# Music Source Separation by Pretrained Models\n","If you want to separate your own music files, see\n","- `egs/tutorials/conv-tasnet/separate_music.ipynb` for Conv-TasNet\n","- `egs/tutorials/mm-dense-lstm/separate_music.ipynb` for MMDenseLSTM\n","- `egs/tutorials/umx/separate_music.ipynb` for OpenUnmix\n","- `egs/tutorials/d3net/separate_music.ipynb` for D3Net"]},{"cell_type":"code","metadata":{"id":"eiAAeD-sFW9m"},"source":["%%shell\n","git clone https://github.com/tky823/DNN-based_source_separation.git\n","\n","cd \"./DNN-based_source_separation/egs/tutorials/\"\n","\n","# To install torch & torchaudio\n","pip install -r requirements.txt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5hRxppAZFvKf"},"source":["%%shell\n","# Download music datset\n","wget \"https://zenodo.org/api/files/1ff52183-071a-4a59-923f-7a31c4762d43/MUSDB18-7-STEMS.zip\"\n","unzip \"./MUSDB18-7-STEMS.zip\"\n","\n","# Convert .mp4 to .wav\n","cd \"./train\"\n","\n","for stem in *.stem.mp4 ; do\n","    name=`echo $stem | awk -F\".stem.mp4\" '{$0=$1}1'`;\n","    echo \"$stem\"\n","    mkdir \"$name\"\n","    cd \"$name\"\n","    ffmpeg -loglevel panic -i \"../${stem}\" -map 0:0 -vn mixture.wav\n","    ffmpeg -loglevel panic -i \"../${stem}\" -map 0:1 -vn drums.wav\n","    ffmpeg -loglevel panic -i \"../${stem}\" -map 0:2 -vn bass.wav\n","    ffmpeg -loglevel panic -i \"../${stem}\" -map 0:3 -vn other.wav\n","    ffmpeg -loglevel panic -i \"../${stem}\" -map 0:4 -vn vocals.wav\n","    cd \"../\"\n","done"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Sm21an1_GwgT"},"source":["import sys\n","sys.path.append(\"/content/DNN-based_source_separation/src\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2MoIuAY2GwdY"},"source":["import IPython.display as ipd\n","import torch\n","import torchaudio"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L2xEU6rUGway"},"source":["from models.conv_tasnet import ConvTasNet\n","from models.mm_dense_lstm import MMDenseLSTM, ParallelMMDenseLSTM\n","from models.umx import OpenUnmix, ParallelOpenUnmix\n","from models.xumx import CrossNetOpenUnmix\n","from models.d3net import D3Net, ParallelD3Net"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HX2S7-B1IE1M"},"source":["n_sources = 4\n","name = \"ANiMAL - Rockshow\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RqRAt917G0iZ"},"source":["waveform_bass, sample_rate = torchaudio.load(\"/content/train/{}/bass.wav\".format(name))\n","waveform_drums, sample_rate = torchaudio.load(\"/content/train/{}/drums.wav\".format(name))\n","waveform_other, sample_rate = torchaudio.load(\"/content/train/{}/other.wav\".format(name))\n","waveform_vocals, sample_rate = torchaudio.load(\"/content/train/{}/vocals.wav\".format(name))\n","\n","display(ipd.Audio(waveform_bass, rate=sample_rate))\n","display(ipd.Audio(waveform_drums, rate=sample_rate))\n","display(ipd.Audio(waveform_other, rate=sample_rate))\n","display(ipd.Audio(waveform_vocals, rate=sample_rate))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JSTTjIzoHzLx"},"source":["mixture = waveform_bass + waveform_drums + waveform_other + waveform_vocals\n","display(ipd.Audio(mixture, rate=sample_rate))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YS1DAoOgHWRA"},"source":["model = ConvTasNet.build_from_pretrained(task=\"musdb18\", sample_rate=sample_rate)\n","\n","input = mixture.unsqueeze(dim=0).unsqueeze(dim=1)\n","\n","model.eval()\n","with torch.no_grad():\n","    output = model(input)\n","\n","output = output.squeeze(dim=0)\n","estimated = torch.unbind(output, dim=0)\n","\n","for idx in range(n_sources):\n","    display(ipd.Audio(estimated[idx], rate=sample_rate))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VKyGXPezbVaS"},"source":["modules = {}\n","modules['bass'] = MMDenseLSTM.build_from_pretrained(task=\"musdb18\", sample_rate=sample_rate, target='bass')\n","modules['drums'] = MMDenseLSTM.build_from_pretrained(task=\"musdb18\", sample_rate=sample_rate, target='drums')\n","modules['other'] = MMDenseLSTM.build_from_pretrained(task=\"musdb18\", sample_rate=sample_rate, target='other')\n","modules['vocals'] = MMDenseLSTM.build_from_pretrained(task=\"musdb18\", sample_rate=sample_rate, target='vocals')\n","model = ParallelMMDenseLSTM(modules)\n","wrapper_model = ParallelMMDenseLSTM.TimeDomainWrapper(model, fft_size=4096, hop_size=1024, window_fn='hann')\n","\n","input = mixture.unsqueeze(dim=0).unsqueeze(dim=1)\n","\n","wrapper_model.eval()\n","with torch.no_grad():\n","    output = wrapper_model(input)\n","\n","output = output.squeeze(dim=0)\n","estimated = torch.unbind(output, dim=0)\n","\n","for idx in range(n_sources):\n","    display(ipd.Audio(estimated[idx], rate=sample_rate))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ep7z1AtCZPyC"},"source":["modules = {}\n","modules['bass'] = OpenUnmix.build_from_pretrained(task=\"musdb18\", sample_rate=sample_rate, target='bass')\n","modules['drums'] = OpenUnmix.build_from_pretrained(task=\"musdb18\", sample_rate=sample_rate, target='drums')\n","modules['other'] = OpenUnmix.build_from_pretrained(task=\"musdb18\", sample_rate=sample_rate, target='other')\n","modules['vocals'] = OpenUnmix.build_from_pretrained(task=\"musdb18\", sample_rate=sample_rate, target='vocals')\n","model = ParallelOpenUnmix(modules)\n","wrapper_model = ParallelOpenUnmix.TimeDomainWrapper(model, fft_size=4096, hop_size=1024, window_fn='hann')\n","\n","input = mixture.unsqueeze(dim=0).unsqueeze(dim=1)\n","\n","wrapper_model.eval()\n","with torch.no_grad():\n","    output = wrapper_model(input)\n","\n","output = output.squeeze(dim=0)\n","estimated = torch.unbind(output, dim=0)\n","\n","for idx in range(n_sources):\n","    display(ipd.Audio(estimated[idx], rate=sample_rate))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Zidt4JLZbpGx"},"source":["model = CrossNetOpenUnmix.build_from_pretrained(task=\"musdb18\", sample_rate=sample_rate)\n","wrapper_model = CrossNetOpenUnmix.TimeDomainWrapper(model, fft_size=4096, hop_size=1024, window_fn='hann')\n","\n","input = mixture.unsqueeze(dim=0).unsqueeze(dim=1)\n","\n","wrapper_model.eval()\n","with torch.no_grad():\n","    output = wrapper_model(input)\n","\n","output = output.squeeze(dim=0)\n","estimated = torch.unbind(output, dim=0)\n","\n","for idx in range(n_sources):\n","    display(ipd.Audio(estimated[idx], rate=sample_rate))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1AYrSVEUYU2H"},"source":["modules = {}\n","modules['bass'] = D3Net.build_from_pretrained(task=\"musdb18\", sample_rate=sample_rate, target='bass')\n","modules['drums'] = D3Net.build_from_pretrained(task=\"musdb18\", sample_rate=sample_rate, target='drums')\n","modules['other'] = D3Net.build_from_pretrained(task=\"musdb18\", sample_rate=sample_rate, target='other')\n","modules['vocals'] = D3Net.build_from_pretrained(task=\"musdb18\", sample_rate=sample_rate, target='vocals')\n","model = ParallelD3Net(modules)\n","wrapper_model = ParallelD3Net.TimeDomainWrapper(model, fft_size=4096, hop_size=1024, window_fn='hann')\n","\n","input = mixture.unsqueeze(dim=0).unsqueeze(dim=1)\n","\n","wrapper_model.eval()\n","with torch.no_grad():\n","    output = wrapper_model(input)\n","\n","output = output.squeeze(dim=0)\n","estimated = torch.unbind(output, dim=0)\n","\n","for idx in range(n_sources):\n","    display(ipd.Audio(estimated[idx], rate=sample_rate))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FuneODf-ccQF"},"source":[""],"execution_count":null,"outputs":[]}]}