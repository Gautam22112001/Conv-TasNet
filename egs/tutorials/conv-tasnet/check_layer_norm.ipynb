{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"check_layer_norm.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyORTHYmJRsJzSAD8aPy9qk0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"CyJXFJPsUnFP"},"source":["import numpy as np\n","import torch\n","import torch.nn as nn"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mWIgGxfZUrY-"},"source":["EPS = 1e-8"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7J7alufVURu3"},"source":["\"\"\"\n","    Based on https://github.com/naplab/Conv-TasNet/blob/master/utility/models.py#L8-L45\n","\"\"\"\n","class OriginalCumlativeLayerNorm(nn.Module):\n","    def __init__(self, dimension, eps=EPS, trainable=True):\n","        super(OriginalCumlativeLayerNorm, self).__init__()\n","        \n","        self.eps = eps\n","        if trainable:\n","            self.gain = nn.Parameter(torch.ones(1, dimension, 1))\n","            self.bias = nn.Parameter(torch.zeros(1, dimension, 1))\n","        else:\n","            self.gain = Variable(torch.ones(1, dimension, 1), requires_grad=False)\n","            self.bias = Variable(torch.zeros(1, dimension, 1), requires_grad=False)\n","\n","    def forward(self, input):\n","        # input size: (Batch, Freq, Time)\n","        # cumulative mean for each time step\n","        \n","        batch_size = input.size(0)\n","        channel = input.size(1)\n","        time_step = input.size(2)\n","        \n","        step_sum = input.sum(1)  # B, T\n","        step_pow_sum = input.pow(2).sum(1)  # B, T\n","        cum_sum = torch.cumsum(step_sum, dim=1)  # B, T\n","        cum_pow_sum = torch.cumsum(step_pow_sum, dim=1)  # B, T\n","        \n","        entry_cnt = np.arange(channel, channel*(time_step+1), channel)\n","        entry_cnt = torch.from_numpy(entry_cnt).type(input.type())\n","        entry_cnt = entry_cnt.view(1, -1).expand_as(cum_sum)\n","        \n","        cum_mean = cum_sum / entry_cnt  # B, T\n","        cum_var = (cum_pow_sum - 2*cum_mean*cum_sum) / entry_cnt + cum_mean.pow(2)  # B, T\n","        cum_std = (cum_var + self.eps).sqrt()  # B, T\n","        \n","        cum_mean = cum_mean.unsqueeze(1)\n","        cum_std = cum_std.unsqueeze(1)\n","        \n","        x = (input - cum_mean.expand_as(input)) / cum_std.expand_as(input)\n","\n","        return x * self.gain.expand_as(x).type(x.type()) + self.bias.expand_as(x).type(x.type())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MAj3KnfhVBL0"},"source":["\"\"\"\n","    Based on https://github.com/tky823/DNN-based_source_separation/blob/main/src/norm.py#L42-L103\n","\"\"\"\n","class MyCumulativeLayerNorm(nn.Module):\n","    def __init__(self, num_features, eps=EPS):\n","        super().__init__()\n","        \n","        self.num_features = num_features\n","        self.eps = eps\n","\n","        self.gamma = nn.Parameter(torch.Tensor(1, num_features, 1))\n","        self.beta = nn.Parameter(torch.Tensor(1, num_features, 1))\n","        \n","        self._reset_parameters()\n","        \n","    def _reset_parameters(self):\n","        self.gamma.data.fill_(1)\n","        self.beta.data.zero_()\n","        \n","    def forward(self, input):\n","        \"\"\"\n","        Args:\n","            input (batch_size, C, T) or (batch_size, C, S, chunk_size):\n","        Returns:\n","            output (batch_size, C, T) or (batch_size, C, S, chunk_size): same shape as the input\n","        \"\"\"\n","        eps = self.eps\n","\n","        n_dim = input.dim()\n","\n","        if n_dim == 3:\n","            batch_size, C, T = input.size()\n","        elif n_dim == 4:\n","            batch_size, C, S, chunk_size = input.size()\n","            T = S * chunk_size\n","            input = input.view(batch_size, C, T)\n","        else:\n","            raise ValueError(\"Only support 3D or 4D input, but given {}D\".format(input.dim()))\n","        \n","        step_sum = input.sum(dim=1) # -> (batch_size, T)\n","        input_pow = input**2\n","        step_pow_sum = input_pow.sum(dim=1) # -> (batch_size, T)\n","        cum_sum = torch.cumsum(step_sum, dim=1) # -> (batch_size, T)\n","        cum_squared_sum = torch.cumsum(step_pow_sum, dim=1) # -> (batch_size, T)\n","        \n","        cum_num = torch.arange(C, C*(T+1), C, dtype=torch.float) # -> (T, ): [C, 2*C, ..., T*C]\n","        cum_mean = cum_sum / cum_num # (batch_size, T)\n","        cum_squared_mean = cum_squared_sum / cum_num\n","        cum_var = cum_squared_mean - cum_mean**2\n","        \n","        cum_mean = cum_mean.unsqueeze(dim=1)\n","        cum_var = cum_var.unsqueeze(dim=1)\n","        \n","        output = (input - cum_mean) / (torch.sqrt(cum_var) + eps) * self.gamma + self.beta\n","\n","        if n_dim == 4:\n","            output = output.view(batch_size, C, S, chunk_size)\n","        \n","        return output\n","    \n","    def __repr__(self):\n","        s = '{}'.format(self.__class__.__name__)\n","        s += '({num_features}, eps={eps})'\n","        \n","        return s.format(**self.__dict__)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GTXKSfxKVo9i"},"source":["torch.manual_seed(111)\n","B, C, T = 4, 3, 10"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9CoVr471VVPN"},"source":["original_layer_norm = OriginalCumlativeLayerNorm(C)\n","my_layer_norm = MyCumulativeLayerNorm(C)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qNvXQJgNVhxD"},"source":["input = torch.randn(B, C, T)\n","original_output = original_layer_norm(input)\n","my_output = my_layer_norm(input)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RNXQlpxVV40s"},"source":["torch.allclose(original_output, my_output)"],"execution_count":null,"outputs":[]}]}